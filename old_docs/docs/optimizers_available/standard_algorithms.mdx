---
title: Standard algorithms
sidebar_position: 1
---

<!-- Currently, modOpt has a fully transparent library of optimization algorithms 
implemented for different types of optimization problems.
The following are the algorithms implemented:

### 1. Steepest-Descent
The implementation can be found 
**[here](https://github.com/LSDOlab/modopt/blob/main/modopt/core/optimization_algorithms/steepest_descent.py)**.

### 2. Newton
The implementation can be found 
**[here](https://github.com/LSDOlab/modopt/blob/main/modopt/core/optimization_algorithms/newton.py)**.


### 3. Quasi-Newton
The implementation can be found 
**[here](https://github.com/LSDOlab/modopt/blob/main/modopt/core/optimization_algorithms/quasi_newton.py)**.

### 4. Newton-Lagrange
The implementation can be found 
**[here](https://github.com/LSDOlab/modopt/blob/main/modopt/core/optimization_algorithms/newton_lagrange.py)**.

### 5. l2-Penalty
The implementation can be found 
**[here](https://github.com/LSDOlab/modopt/blob/main/modopt/core/optimization_algorithms/quadratic_penalty_eq.py)**. -->

## Usage instructions
In order to use the standard algorithms with any of the  problems written using
the **Problem()** class, you should first import your problem from the corresponding file
and also import the optimizer of your choice from the library.
After that, set tolerances and other parameters for the chosen optimizer.
Solve the problem and then print results.

An example is shown below for the SteepestDescent optimizer.

```py

from my_problem import MyProblem

from modopt import SteepestDescent, Newton, QuasiNewton

tol = 1E-8
max_itr = 500

prob = MyProblem()

optimizer = SteepestDescent(prob,opt_tol=tol,max_itr=max_itr)
# optimizer = Newton(prob,opt_tol=tol,max_itr=max_itr)
# optimizer = QuasiNewton(prob,opt_tol=tol,max_itr=max_itr)

optimizer.check_first_derivatives(prob.x.get_data())
optimizer.solve()
optimizer.print_results(summary_table=True)
```